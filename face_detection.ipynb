{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Imports and Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q labelme bs4 tensorflow opencv-python-headless matplotlib albumentations scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 13:03:32.455430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-02 13:03:32.575728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/envs/facedet/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2025-01-02 13:03:32.575749: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-02 13:03:32.610162: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-02 13:03:33.112739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/envs/facedet/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2025-01-02 13:03:33.112835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/envs/facedet/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2025-01-02 13:03:33.112847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import albumentations as alb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Directory Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character classes\n",
    "CHARACTER_CLASSES = {\n",
    "    \"Sheldon\": 0,\n",
    "    \"Leonard\": 1,\n",
    "    \"Penny\": 2,\n",
    "    \"Howard\": 3,\n",
    "    \"Raj\": 4,\n",
    "    \"Amy\": 5,\n",
    "    \"Bernadette\": 6\n",
    "}\n",
    "CHARACTER_NAMES = {v: k for k, v in CHARACTER_CLASSES.items()}  # Reverse mapping\n",
    "\n",
    "# Define paths\n",
    "IMG_DIR = \"data/images\"\n",
    "LABELS_DIR = \"data/labels\"\n",
    "TRAIN_IMAGES_DIR = os.path.join(\"data\", \"train\", \"images\")\n",
    "TRAIN_LABELS_DIR = os.path.join(\"data\", \"train\", \"labels\")\n",
    "VALIDATION_IMAGES_DIR = os.path.join(\"data\", \"validation\", \"images\")\n",
    "VALIDATION_LABELS_DIR = os.path.join(\"data\", \"validation\", \"labels\")\n",
    "TEST_IMAGES_DIR = os.path.join(\"data\", \"test\", \"images\")\n",
    "TEST_LABELS_DIR = os.path.join(\"data\", \"test\", \"labels\")\n",
    "\n",
    "\n",
    "# Define agumentation paths\n",
    "AUG_DATA_DIR = \"aug_data\"\n",
    "TRAIN_AUG_IMAGES_DIR = os.path.join(AUG_DATA_DIR, \"train\", \"images\")\n",
    "TRAIN_AUG_LABELS_DIR = os.path.join(AUG_DATA_DIR, \"train\", \"labels\")\n",
    "VALIDATION_AUG_IMAGES_DIR = os.path.join(AUG_DATA_DIR, \"validation\", \"images\")\n",
    "VALIDATION_AUG_LABELS_DIR = os.path.join(AUG_DATA_DIR, \"validation\", \"labels\")\n",
    "TEST_AUG_IMAGES_DIR = os.path.join(AUG_DATA_DIR, \"test\", \"images\")\n",
    "TEST_AUG_LABELS_DIR = os.path.join(AUG_DATA_DIR, \"test\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary directories exist\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "os.makedirs(LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(VALIDATION_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(VALIDATION_LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_LABELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "os.makedirs(AUG_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_AUG_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_AUG_LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(VALIDATION_AUG_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(VALIDATION_AUG_LABELS_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_AUG_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_AUG_LABELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Setting up functions and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for each character\n",
    "CHARACTER_URLS = {\n",
    "    \"Sheldon\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm1433588\",\n",
    "    \"Leonard\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm0301959\",\n",
    "    \"Penny\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm0192505\",\n",
    "    \"Howard\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm0374865\",\n",
    "    \"Raj\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm2471798\",\n",
    "    \"Amy\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm0080524\",\n",
    "    \"Bernadette\": \"https://www.imdb.com/title/tt0898266/mediaindex/?relatedNames=nm1851981\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers to simulate a browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_directory(base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_urls, save_path, limit=30, prefix=\"\"):\n",
    "    for i, url in enumerate(image_urls[:limit]):\n",
    "        try:\n",
    "            unique_filename = f\"{prefix}_{uuid.uuid4()}.jpg\"\n",
    "            response = requests.get(url, headers=HEADERS, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(os.path.join(save_path, unique_filename), \"wb\") as file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        file.write(chunk)\n",
    "            else:\n",
    "                print(f\"Error accessing image {i + 1}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {i + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_urls(imdb_url):\n",
    "    try:\n",
    "        response = requests.get(imdb_url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        images = soup.find_all('img')\n",
    "        image_urls = [\n",
    "            img['src']\n",
    "            for img in images\n",
    "            if 'src' in img.attrs and 'media' in img['src']\n",
    "        ]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting URLs from {imdb_url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_images():\n",
    "    create_base_directory(IMG_DIR)\n",
    "    for character, url in CHARACTER_URLS.items():\n",
    "        print(f\"Downloading images for {character}...\")\n",
    "        image_urls = get_image_urls(url)\n",
    "        if not image_urls:\n",
    "            print(f\"No images found for {character}.\")\n",
    "            continue\n",
    "        download_images(image_urls, IMG_DIR, prefix=character)\n",
    "        print(f\"{character}: Images downloaded successfully!\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Collecting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Annotate Images with LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-31 18:11:03.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlabelme.config\u001b[0m:\u001b[36mget_config\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading config file from: C:\\Users\\welli\\.labelmerc\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition Unaugmented Data into Train, Validation, and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(img_dir, labels_dir, train_dir, val_dir, test_dir):\n",
    "    \"\"\"\n",
    "    Split dataset into training, validation, and test directories for both images and labels.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Path to the directory containing images.\n",
    "        labels_dir (str): Path to the directory containing corresponding labels.\n",
    "        train_dir (str): Path to the directory to store training images and labels.\n",
    "        val_dir (str): Path to the directory to store validation images and labels.\n",
    "        test_dir (str): Path to the directory to store test images and labels.\n",
    "    \"\"\"\n",
    "    # List all image files\n",
    "    images = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    # Split data into training (70%), validation (15%), and test (15%)\n",
    "    train_images, temp_images = train_test_split(images, test_size=0.3, random_state=42)\n",
    "    val_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Helper function to move images and their corresponding labels\n",
    "    def move_files(file_list, src_img_dir, src_label_dir, dest_img_dir, dest_label_dir):\n",
    "        for img in file_list:\n",
    "            # Derive corresponding label filename\n",
    "            label_file = img.replace('.jpg', '.json')\n",
    "            label_path = os.path.join(src_label_dir, label_file)\n",
    "\n",
    "            # Check if label exists; if not, skip moving the image\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Skipping {img} as no corresponding label was found.\")\n",
    "                continue\n",
    "\n",
    "            # Move image file\n",
    "            img_path = os.path.join(src_img_dir, img)\n",
    "            dest_img_path = os.path.join(dest_img_dir, img)\n",
    "            os.makedirs(dest_img_dir, exist_ok=True)  # Ensure destination exists\n",
    "            os.rename(img_path, dest_img_path)\n",
    "\n",
    "            # Move label file\n",
    "            dest_label_path = os.path.join(dest_label_dir, label_file)\n",
    "            os.makedirs(dest_label_dir, exist_ok=True)  # Ensure destination exists\n",
    "            os.rename(label_path, dest_label_path)\n",
    "\n",
    "    # Move training data\n",
    "    move_files(train_images, img_dir, labels_dir, train_dir, train_dir.replace('images', 'labels'))\n",
    "\n",
    "    # Move validation data\n",
    "    move_files(val_images, img_dir, labels_dir, val_dir, val_dir.replace('images', 'labels'))\n",
    "\n",
    "    # Move test data\n",
    "    move_files(test_images, img_dir, labels_dir, test_dir, test_dir.replace('images', 'labels'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Raj_2da5b52a-ad4b-4951-94e1-510ea0b7844d.jpg as no corresponding label was found.\n",
      "Skipping Raj_3c48c87f-ba72-4a4e-a550-c9c5c2c1560e.jpg as no corresponding label was found.\n",
      "Skipping Leonard_b5b7e572-5645-48b6-971f-875fc5d8f5dd.jpg as no corresponding label was found.\n",
      "Skipping Amy_1b70375d-3d08-4df5-8edd-d2da4b354d24.jpg as no corresponding label was found.\n",
      "Skipping Raj_779fccea-45db-4305-afeb-8ef0ddc9ed0a.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_25d3b478-5854-4c54-8dc2-98588f6bd40f.jpg as no corresponding label was found.\n",
      "Skipping Howard_bf54bdc9-0d25-4265-b3a2-e5b257427ce0.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_96e884f6-e4c5-442e-b73a-02eedd37720f.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_0b4bba54-e8a5-4844-ba21-4d8379498f78.jpg as no corresponding label was found.\n",
      "Skipping Leonard_6ce2c3b9-4b6b-4c57-b456-e9d94924c3a7.jpg as no corresponding label was found.\n",
      "Skipping Penny_8c54af65-ea20-46d7-869a-8c9948ad233f.jpg as no corresponding label was found.\n",
      "Skipping Leonard_2d03206e-427b-470b-b51f-caabb009a870.jpg as no corresponding label was found.\n",
      "Skipping Raj_fab991bf-a609-4a30-b5d0-987ba613a459.jpg as no corresponding label was found.\n",
      "Skipping Raj_fa8f70e4-5def-40e5-aa94-6d9a3b3875ba.jpg as no corresponding label was found.\n",
      "Skipping Howard_71f5d631-a3a9-45a4-8891-eab5e2c261b1.jpg as no corresponding label was found.\n",
      "Skipping Penny_d34b4615-ec79-4f98-8e78-a0410c0e9a7a.jpg as no corresponding label was found.\n",
      "Skipping Howard_13851a41-e3bd-48c3-a103-14a8210b0895.jpg as no corresponding label was found.\n",
      "Skipping Howard_28978f85-38ed-4764-ad7e-326a2dac290d.jpg as no corresponding label was found.\n",
      "Skipping Raj_0bac0836-b3e5-46ae-a77e-2fb82609d014.jpg as no corresponding label was found.\n",
      "Skipping Raj_4f3ece8c-ac5e-415e-a29f-83a75436bdd0.jpg as no corresponding label was found.\n",
      "Skipping Howard_7c627730-b5d7-4342-a152-239c57871a56.jpg as no corresponding label was found.\n",
      "Skipping Raj_c4f31602-4ebd-496a-b3e5-22fd0d1aa4d9.jpg as no corresponding label was found.\n",
      "Skipping Raj_5afc0a8e-1aa3-44ce-a20d-7ebf5ecb065e.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_bb1fc05b-339d-407b-8812-87f744d5d42c.jpg as no corresponding label was found.\n",
      "Skipping Leonard_108df1d9-5ffb-4260-969b-d28be1876544.jpg as no corresponding label was found.\n",
      "Skipping Penny_3ebc7488-b569-42bb-ad18-ea31c4017392.jpg as no corresponding label was found.\n",
      "Skipping Raj_8bb3d278-4b56-4040-8550-07c444a932f3.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_8d59d51b-960b-4330-a7dc-e6aa2652ca6b.jpg as no corresponding label was found.\n",
      "Skipping Howard_3dd2f4d2-216f-4b2e-8bf3-4cea8b2da4df.jpg as no corresponding label was found.\n",
      "Skipping Howard_f77079ba-bbe5-418a-af62-a3c6ced92e59.jpg as no corresponding label was found.\n",
      "Skipping Penny_ce38b9d1-1c70-49eb-9ed7-7de42feb0ce4.jpg as no corresponding label was found.\n",
      "Skipping Leonard_00e249c6-9756-44fe-8795-3ac0d7492029.jpg as no corresponding label was found.\n",
      "Skipping Raj_ec989a80-bde6-46f4-9d5c-1c240748dbe0.jpg as no corresponding label was found.\n",
      "Skipping Penny_67c659eb-ae10-446b-9ec5-434702741496.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_b44ba682-7570-4076-8a03-0cd993c67c2e.jpg as no corresponding label was found.\n",
      "Skipping Howard_04d66133-d1bd-492b-af7e-2c934aad6f09.jpg as no corresponding label was found.\n",
      "Skipping Penny_731f734d-c5f9-4f0e-8cb1-38ee74e9b1f3.jpg as no corresponding label was found.\n",
      "Skipping Howard_f6f8ca19-f604-479c-9cda-200d915de553.jpg as no corresponding label was found.\n",
      "Skipping Penny_97bfe722-7e3e-4b79-b7c0-6471f8ea829e.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_74a6a369-fb4e-4f05-9c22-6804b48d473e.jpg as no corresponding label was found.\n",
      "Skipping Leonard_8b9f4a48-beab-488c-9e62-f2c3ff186d4b.jpg as no corresponding label was found.\n",
      "Skipping Howard_451dfaeb-cae7-4e31-8e9c-c34b3cc70411.jpg as no corresponding label was found.\n",
      "Skipping Leonard_a52f4f34-05b6-4a40-8327-39ef95d77cd3.jpg as no corresponding label was found.\n",
      "Skipping Howard_d6924235-82a3-455e-9f53-6745b161bfb5.jpg as no corresponding label was found.\n",
      "Skipping Raj_8c773cd3-179a-4bba-91b0-1d961b19cf23.jpg as no corresponding label was found.\n",
      "Skipping Raj_91ccdc44-1491-4480-880e-f6812f6dd6e9.jpg as no corresponding label was found.\n",
      "Skipping Amy_018f69ee-1518-4575-a1f7-332fc6c4f7a7.jpg as no corresponding label was found.\n",
      "Skipping Leonard_1357bdfe-e284-4560-bcf0-cb772da82718.jpg as no corresponding label was found.\n",
      "Skipping Raj_1fbf1e94-f340-4387-8678-5a01b088b50a.jpg as no corresponding label was found.\n",
      "Skipping Penny_e7336fdd-725f-48e1-920e-9d9da01544dc.jpg as no corresponding label was found.\n",
      "Skipping Penny_09ac9174-84e0-4e46-b8a3-e3597076259f.jpg as no corresponding label was found.\n",
      "Skipping Howard_4aab6f37-2f56-42a6-b2d3-203035114f91.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_ed7e527e-44ee-417d-99ae-a0948653d1fc.jpg as no corresponding label was found.\n",
      "Skipping Penny_7e34706c-cbe7-4f18-952c-93afb01aaa67.jpg as no corresponding label was found.\n",
      "Skipping Penny_6b10b8b2-a186-4d0d-b07e-1781a0c3e1e7.jpg as no corresponding label was found.\n",
      "Skipping Penny_1f7ac7c9-22fc-48c9-a05c-b84aa781d47a.jpg as no corresponding label was found.\n",
      "Skipping Raj_b8f45213-ef8b-4dd3-8143-b7eb146809c3.jpg as no corresponding label was found.\n",
      "Skipping Raj_cd50c591-6c96-4112-b06f-e530c56b2979.jpg as no corresponding label was found.\n",
      "Skipping Raj_9cc867d6-be03-45db-9be5-4673a5ccae8b.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_8f95b631-2704-4896-bdf0-658747d219ac.jpg as no corresponding label was found.\n",
      "Skipping Raj_ebce49fa-0aa7-4a04-9ba4-3c2718767d84.jpg as no corresponding label was found.\n",
      "Skipping Penny_5e59c57b-d4a6-4f1f-b383-537c44aa2ecc.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_603b2067-92cb-4635-ba9d-6abfc461784c.jpg as no corresponding label was found.\n",
      "Skipping Howard_f5e3eade-0050-440b-9173-4ee6b7e1bfb7.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_d432ea90-d4ed-44e9-97b5-bf8cca463745.jpg as no corresponding label was found.\n",
      "Skipping Penny_a5533135-3efc-4104-89b4-8c56e3bf3d96.jpg as no corresponding label was found.\n",
      "Skipping Penny_3ffc1b4e-0b5b-4d75-bef0-3dcfbfc5b7da.jpg as no corresponding label was found.\n",
      "Skipping Raj_04939c6e-e08b-4d69-8e6d-f93dc18bc992.jpg as no corresponding label was found.\n",
      "Skipping Howard_360c120d-431d-4855-9b72-a7d06feab4c5.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_67fcf223-3de0-49ac-aaec-2e7fa8234591.jpg as no corresponding label was found.\n",
      "Skipping Leonard_7f0deaf0-5527-4aa6-9b5e-ef2198b2f7e4.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_fc88a47e-4a95-4dd1-bb29-9f6eb1c0a175.jpg as no corresponding label was found.\n",
      "Skipping Leonard_229804f1-c512-4741-bf0b-ce6a7b0ed7e7.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_3f572d58-57a1-425a-a487-ca7f5391b421.jpg as no corresponding label was found.\n",
      "Skipping Leonard_fe33bed3-f260-4507-a235-d0bfe8e52842.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_d286b8a2-dd96-4e14-a5b0-10bafac2d2d5.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_82af066c-a5e5-4c84-8d9b-da0c87a1ed8b.jpg as no corresponding label was found.\n",
      "Skipping Penny_2f11fc57-c36d-4b83-9f18-2b3ebb291487.jpg as no corresponding label was found.\n",
      "Skipping Penny_be7fe0bf-0469-46a8-a5a3-9af2a4fbbeac.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_3f0f74da-f42a-4fb4-abd3-fb93388c95e2.jpg as no corresponding label was found.\n",
      "Skipping Leonard_1243c81d-021d-40f8-9b36-1fb4c6a35399.jpg as no corresponding label was found.\n",
      "Skipping Penny_c8493656-3b0c-45f5-b8f1-12e80a0deb41.jpg as no corresponding label was found.\n",
      "Skipping Raj_ed19ab69-66c6-4508-b8d2-b906c15022cb.jpg as no corresponding label was found.\n",
      "Skipping Penny_c2db236b-ffbe-4d39-a978-1272537e2b08.jpg as no corresponding label was found.\n",
      "Skipping Penny_367c88f9-db37-4659-bedb-081b11ed8fba.jpg as no corresponding label was found.\n",
      "Skipping Leonard_bb617b62-5468-42bd-9180-bb49eaf565c6.jpg as no corresponding label was found.\n",
      "Skipping Penny_0829d24b-a9c1-48d6-b8cd-a046ae7bcc75.jpg as no corresponding label was found.\n",
      "Skipping Leonard_65a4bcf0-9498-43dc-8ae3-f56c73cffe72.jpg as no corresponding label was found.\n",
      "Skipping Raj_146925ed-9d8b-41cf-b111-f3b07c2b791c.jpg as no corresponding label was found.\n",
      "Skipping Raj_36d8455e-90c3-472f-bca4-766bf57dfe73.jpg as no corresponding label was found.\n",
      "Skipping Penny_426b2efe-da37-4ee1-83a1-168586514895.jpg as no corresponding label was found.\n",
      "Skipping Penny_592eafdc-a767-4168-8a5b-b6d756b85260.jpg as no corresponding label was found.\n",
      "Skipping Raj_d096015a-180f-4f54-9271-065d776decb9.jpg as no corresponding label was found.\n",
      "Skipping Raj_bc66c547-88da-43b1-95a5-e7324545b2bd.jpg as no corresponding label was found.\n",
      "Skipping Raj_86a854e8-949d-49a7-bb9a-5a1695b381e5.jpg as no corresponding label was found.\n",
      "Skipping Penny_53e12322-6757-49cc-ae48-67b9c89c1d24.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_b5bc6ebb-096b-400c-943c-6ebe601abcb1.jpg as no corresponding label was found.\n",
      "Skipping Raj_2d0aa4dc-4f11-4d2f-8b5d-22e399291954.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_557c06d8-9b70-4f2a-96b6-207674e2c44f.jpg as no corresponding label was found.\n",
      "Skipping Penny_eb7066b8-a1a6-4196-a507-40f862996041.jpg as no corresponding label was found.\n",
      "Skipping Howard_be2e62fb-d708-47f6-8ea5-4da2c86fe28b.jpg as no corresponding label was found.\n",
      "Skipping Penny_974b7a09-c744-4beb-a359-114bacd6a6d9.jpg as no corresponding label was found.\n",
      "Skipping Raj_eecf2fa0-7a64-47b3-a8f0-247a8dd358eb.jpg as no corresponding label was found.\n",
      "Skipping Sheldon_5908d168-7d77-4bd5-85f4-f875b7816c33.jpg as no corresponding label was found.\n",
      "Skipping Bernadette_3cc9e9d4-089f-48b7-bb49-f75b34d06726.jpg as no corresponding label was found.\n",
      "Skipping Leonard_2325d175-3653-458d-a2a0-ed902368b88a.jpg as no corresponding label was found.\n",
      "Skipping Howard_6bcbfcc4-0a37-4e89-b4a2-fbfc48aae0c7.jpg as no corresponding label was found.\n",
      "Skipping Penny_7295ebc5-a202-488c-b91d-4d35d3214834.jpg as no corresponding label was found.\n"
     ]
    }
   ],
   "source": [
    "# Perform the split\n",
    "split_data(IMG_DIR, LABELS_DIR, TRAIN_IMAGES_DIR, VALIDATION_IMAGES_DIR, TEST_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/facedet/lib/python3.9/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `dict[str, any]` but got `UniformParams` with value `UniformParams(noise_type=...6, 0.0784313725490196)])` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Define augmentation pipeline with bounding box handling\n",
    "augmentor = alb.Compose([\n",
    "    alb.RandomCrop(width=450, height=450), \n",
    "    alb.HorizontalFlip(p=0.5), \n",
    "    alb.RandomBrightnessContrast(p=0.2),\n",
    "    alb.RandomGamma(p=0.2), \n",
    "    alb.RGBShift(p=0.2), \n",
    "    alb.VerticalFlip(p=0.5)], \n",
    "    bbox_params=alb.BboxParams(format='albumentations', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_augmentation_with_random_sample(train_images_dir, train_labels_dir):\n",
    "    \"\"\"\n",
    "    Test augmentation pipeline using a random sample from the training dataset.\n",
    "    \"\"\"\n",
    "    # Select a random image from the training dataset\n",
    "    random_image = os.listdir(train_images_dir)[0]  # Using the first image for simplicity\n",
    "    image_path = os.path.join(train_images_dir, random_image)\n",
    "    label_path = os.path.join(train_labels_dir, random_image.replace('.jpg', '.json'))\n",
    "\n",
    "    # Ensure the label file exists\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Label file not found for image: {random_image}\")\n",
    "        return\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Load the label\n",
    "    with open(label_path, 'r') as f:\n",
    "        label = json.load(f)\n",
    "    coords = label['shapes'][0]['points']\n",
    "    bbox = [coords[0][0] / w, coords[0][1] / h, coords[1][0] / w, coords[1][1] / h]  # Normalize bbox\n",
    "\n",
    "    augmented = augmentor(image=img, bboxes=[bbox], class_labels=['face'])\n",
    "\n",
    "    # Visualize the augmented image and bounding box\n",
    "    aug_img = augmented['image']\n",
    "    aug_bbox = augmented['bboxes'][0]\n",
    "    start_point = (int(aug_bbox[0] * aug_img.shape[1]), int(aug_bbox[1] * aug_img.shape[0]))\n",
    "    end_point = (int(aug_bbox[2] * aug_img.shape[1]), int(aug_bbox[3] * aug_img.shape[0]))\n",
    "    cv2.rectangle(aug_img, start_point, end_point, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Augmented Image\", aug_img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test the augmentation process with a random sample\n",
    "test_augmentation_with_random_sample(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Viewing Dataset, and Build Image Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Limit GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('..\\\\data\\\\face-detection\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'..\\\\data\\\\face-detection\\\\images\\\\Leonard_b015552d-40ce-4dd2-9f82-84b34756afd7.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 69,  74,  70],\n",
       "        [ 70,  75,  71],\n",
       "        [ 71,  76,  72],\n",
       "        ...,\n",
       "        [211, 202, 187],\n",
       "        [200, 191, 176],\n",
       "        [193, 184, 169]],\n",
       "\n",
       "       [[ 69,  74,  70],\n",
       "        [ 70,  75,  71],\n",
       "        [ 71,  76,  72],\n",
       "        ...,\n",
       "        [217, 208, 193],\n",
       "        [206, 197, 182],\n",
       "        [201, 192, 177]],\n",
       "\n",
       "       [[ 69,  74,  70],\n",
       "        [ 70,  75,  71],\n",
       "        [ 71,  76,  72],\n",
       "        ...,\n",
       "        [225, 213, 197],\n",
       "        [218, 206, 190],\n",
       "        [214, 202, 186]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[150, 149, 154],\n",
       "        [158, 157, 162],\n",
       "        [157, 156, 161],\n",
       "        ...,\n",
       "        [ 54,  73, 106],\n",
       "        [ 54,  73, 106],\n",
       "        [ 54,  73, 106]],\n",
       "\n",
       "       [[134, 133, 138],\n",
       "        [164, 163, 168],\n",
       "        [159, 158, 163],\n",
       "        ...,\n",
       "        [ 57,  73, 107],\n",
       "        [ 57,  73, 107],\n",
       "        [ 57,  73, 107]],\n",
       "\n",
       "       [[132, 131, 136],\n",
       "        [175, 174, 179],\n",
       "        [162, 161, 166],\n",
       "        ...,\n",
       "        [ 58,  74, 108],\n",
       "        [ 58,  74, 108],\n",
       "        [ 58,  74, 108]]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 View Raw Images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [604,820,3], [batch]: [547,820,3] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_images \u001b[38;5;241m=\u001b[39m \u001b[43mimage_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4481\u001b[0m, in \u001b[0;36m_NumpyIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 4481\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4478\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4475\u001b[0m     numpy\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   4476\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m numpy\n\u001b[1;32m-> 4478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(to_numpy, \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\welli\\anaconda3\\envs\\facedet\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [604,820,3], [batch]: [547,820,3] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facedet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
